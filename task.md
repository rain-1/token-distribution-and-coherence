For this project we would like to create a tool just a simple script that will run many invocations of an LLM probably v a local VLLM server that we're running but make it general we wanna be able to point to some open AI compatible LM passing temperature parameters and so on anyway we wanna do 100 or 1000 invocations on perhaps open Hermes 2.5 questions would be a good data set to use just generate responses to that and log them all in a JSON file this will all go on hugging face make sure there's a hugging face token available in the end of and load the data set to hugging face Show progress bar like TQBN in the terminal and as it's going we want to calculate a histogram of the distribution of tokens and what we're checking for is tokens are divided into red and blue based on just the literally the tokeniser the Quen 3 tokeniser if it's an odd or an even token and we we want to also run Oh you should support a custom system prompt too So the user prompt will be the questions from Open Hermes the system prompt might be additionally provided and yeah so we also want check coherence and we can on a much smaller amount of data we don't need to do that on as much data it might be 1% of the generated outputs we'll check make that configurable as a flag but umm we will have a coherence rubric and a a separate judge LLM that we will have a config file to specify for basically doing that I will give you more information about how to do the judge later so just write the code modular and don't implement that part yet
